{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 6975813,
     "sourceType": "datasetVersion",
     "datasetId": 4008439
    },
    {
     "sourceId": 123164466,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30579,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Installing required libraries"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install scikit-learn\n",
    "!pip install torch\n",
    "!pip install datasets\n",
    "!pip install evaluate\n",
    "!pip install transformers\n",
    "!pip install sacremoses sentencepiece"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-20T22:26:39.776735Z",
     "iopub.execute_input": "2023-11-20T22:26:39.777105Z",
     "iopub.status.idle": "2023-11-20T22:27:05.234619Z",
     "shell.execute_reply.started": "2023-11-20T22:26:39.777075Z",
     "shell.execute_reply": "2023-11-20T22:27:05.233533Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### If there is any warnings, it will be ignored"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2023-11-20T22:27:05.236555Z",
     "iopub.execute_input": "2023-11-20T22:27:05.236821Z",
     "iopub.status.idle": "2023-11-20T22:27:05.241542Z",
     "shell.execute_reply.started": "2023-11-20T22:27:05.236795Z",
     "shell.execute_reply": "2023-11-20T22:27:05.240687Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Preparing dataset for train the model\n",
    "\n",
    "In this class, we will use AraBERTv02 tokenizer for Arabic language and RoBERTa tokenizer for English language"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class EnsembleDataset(Dataset):\n",
    "    def __init__(self, text_data, labels, lang='en'):\n",
    "        self.text_data = text_data\n",
    "        self.labels = labels\n",
    "        self.bertar = AutoTokenizer.from_pretrained('aubmindlab/bert-base-arabertv02') # Loading AraBERT tokenizer for Arabic language\n",
    "        self.berten = AutoTokenizer.from_pretrained('roberta-base') # Loading RoBERTa tokenizer for english\n",
    "        self.lang = lang\n",
    "        # one tokenizer will be used at a time for input language\n",
    "        self.lsm = {'ar': self.bertar,\n",
    "                    'en': self.berten\n",
    "                    }\n",
    "        # multilingual BERT tokenizer will be used for both Arabic and English languages\n",
    "        self.mbert = AutoTokenizer.from_pretrained('bert-base-multilingual-uncased') # Loading multilingual BERT tokenizer for both languages\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.text_data[index] # text data based on the index\n",
    "        label = self.labels[index] # corrosponding label of the text data\n",
    "        # tokenize text data using language specific tokenizer\n",
    "        # maximum token length for tokenizer is 256. If the text has more than 256 tokens, it will be trancated\n",
    "        # after tokenize the text, the return format of text will be torch.tensor()\n",
    "        self_tok = self.lsm[self.lang].encode_plus(text, add_special_tokens=True,\n",
    "                                                   max_length=256, padding='max_length',\n",
    "                                                   return_attention_mask=True, truncation=True, return_tensors='pt')\n",
    "        # tokenize text data for both languages\n",
    "        # maximum token length for tokenizer is 256. If the text has more than 256 tokens, it will be trancated\n",
    "        # after tokenize the text, the return format of text will be torch.tensor()\n",
    "        mbert_tok = self.mbert.encode_plus(text, add_special_tokens=True,\n",
    "                                           max_length=256, padding='max_length',\n",
    "                                           return_attention_mask=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "        fdata = (self_tok['input_ids'].squeeze(0), self_tok['attention_mask'].squeeze(0),\n",
    "                 mbert_tok['input_ids'].squeeze(0), mbert_tok['attention_mask'].squeeze(0), torch.tensor(label))\n",
    "        # the function will return input ids and attention masks of each tokenizer and labels will be converted to tensor values\n",
    "        return fdata\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-20T22:27:05.242726Z",
     "iopub.execute_input": "2023-11-20T22:27:05.242985Z",
     "iopub.status.idle": "2023-11-20T22:27:08.608210Z",
     "shell.execute_reply.started": "2023-11-20T22:27:05.242962Z",
     "shell.execute_reply": "2023-11-20T22:27:08.607426Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Defining the proposed ensemble model"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from transformers import AutoModel\n",
    "\n",
    "# Define the Ensemble classification model\n",
    "class EnsembleClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(EnsembleClassifier, self).__init__()\n",
    "\n",
    "        # Loading pretrained langguage models\n",
    "        # AraBERTv02 is for Arabic language and RoBERTa for English\n",
    "        self.bertar = AutoModel.from_pretrained('aubmindlab/bert-base-arabertv02') # ArabicBERT\n",
    "        self.berten = AutoModel.from_pretrained('roberta-base') # Loading RoBERTa for English\n",
    "\n",
    "        # lang specific model\n",
    "        self.lsm = {'ar': self.bertar,\n",
    "                    'en': self.berten\n",
    "                    }\n",
    "        \n",
    "        self.lsm_drop = nn.Dropout(0.1) # language specific dropout\n",
    "\n",
    "        #multilingual BERT for both languages\n",
    "        self.mbert = AutoModel.from_pretrained('bert-base-multilingual-uncased')\n",
    "        self.mbert_drop = nn.Dropout(0.3)\n",
    "        \n",
    "\n",
    "        # Fusion layer\n",
    "        self.fusion_fc = nn.Linear(768*2, 128)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_fc = nn.Linear(128, num_classes)\n",
    "        # Activation function\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        #self.post_init()\n",
    "\n",
    "    def forward(self, lang, in_1, attn_1, in_2, attn_2):\n",
    "        \n",
    "        # we'll feed the input_ids and attention masks to the model\n",
    "        # we'll use pooler output for fusion and feeding into the feed forward network\n",
    "        lsm_output = self.lsm[lang](input_ids=in_1, attention_mask=attn_1)\n",
    "        lsm_output = self.lsm_drop(lsm_output.pooler_output)\n",
    "\n",
    "        mbert_output = self.mbert(in_2, attn_2) #output_hidden_states=True,return_dict=False\n",
    "        mbert_output = self.mbert_drop(mbert_output.pooler_output)\n",
    "\n",
    "        # Concatenate the pooler output from both models\n",
    "        features = torch.cat((lsm_output, lsm_output), dim=1) \n",
    "\n",
    "        # Fusion layer\n",
    "        features = self.fusion_fc(features)\n",
    "\n",
    "        output = self.output_fc(features)\n",
    "        output = self.softmax(output)\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-20T22:27:08.610220Z",
     "iopub.execute_input": "2023-11-20T22:27:08.610616Z",
     "iopub.status.idle": "2023-11-20T22:27:08.871934Z",
     "shell.execute_reply.started": "2023-11-20T22:27:08.610589Z",
     "shell.execute_reply": "2023-11-20T22:27:08.871202Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Each time we'll feed 24 instances into the model. So defining the batch size 24"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "BATCH_SIZE = 24"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-20T22:27:08.873017Z",
     "iopub.execute_input": "2023-11-20T22:27:08.873295Z",
     "iopub.status.idle": "2023-11-20T22:27:08.877199Z",
     "shell.execute_reply.started": "2023-11-20T22:27:08.873271Z",
     "shell.execute_reply": "2023-11-20T22:27:08.876414Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training, Validation, and Evaluation loop"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import gc\n",
    "# Define the training and testing and Evaluation functions\n",
    "def train(model, train_loader, criterion, optimizer, device, lang='en'):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    i=1\n",
    "    for input_1, attn_1, input_2, attn_2, labels in train_loader:\n",
    "        #torch.cuda.empty_cache()\n",
    "        optimizer.zero_grad()\n",
    "        # copy the values of each batch to the available device (CPU or GPU)\n",
    "        input_1 = input_1.to(device)\n",
    "        input_2 = input_2.to(device)\n",
    "        attn_1 = attn_1.to(device)\n",
    "        attn_2 = attn_2.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # passing to the model\n",
    "        output = model(lang, input_1, attn_1, input_2, attn_2)\n",
    "        # calculating the loss for a batch\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step() # stepping the optimizer\n",
    "        train_loss += loss.item() * labels.size(0) # \n",
    "        _, predicted = torch.max(output, 1) # getting the prediction\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        if i % 100 == 0:\n",
    "          loss, current = loss.item(), i * BATCH_SIZE\n",
    "          print(f'{current}/{len(train_loader.dataset)}\\tTrain Loss: {train_loss/(i*BATCH_SIZE):.3f} | Train Acc: {(correct/(i*BATCH_SIZE))*100:.2f}%')\n",
    "        i+=1\n",
    "    return train_loss, correct\n",
    "\n",
    "def test(model, test_loader, criterion, device, lang='en'):\n",
    "    \"\"\"\n",
    "    This function we'll use to evaluate the performance on development data\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for input_1, attn_1, input_2, attn_2, labels in test_loader:\n",
    "            #optimizer.zero_grad()\n",
    "            # copy the values of each batch to the available device (CPU or GPU)\n",
    "            input_1 = input_1.to(device)\n",
    "            input_2 = input_2.to(device)\n",
    "            attn_1 = attn_1.to(device)\n",
    "            attn_2 = attn_2.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # passing to the model\n",
    "            output = model(lang, input_1, attn_1, input_2, attn_2)\n",
    "            loss = criterion(output, labels)\n",
    "            test_loss += loss.item() * labels.size(0)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return test_loss, correct\n",
    "\n",
    "def evaluate(model, test_loader, device, ep, g_labels):\n",
    "    \"\"\"\n",
    "    This function we'll use to evaluate the performance on test data\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    y_test_pred = []\n",
    "    en_test_loader, ar_test_loader = test_loader[0], test_loader[1]\n",
    "    with torch.no_grad():\n",
    "        # for English data\n",
    "        for input_1, attn_1, input_2, attn_2, labels in en_test_loader:\n",
    "            #optimizer.zero_grad()\n",
    "            input_1 = input_1.to(device)\n",
    "            input_2 = input_2.to(device)\n",
    "            attn_1 = attn_1.to(device)\n",
    "            attn_2 = attn_2.to(device)\n",
    "            labels = labels.to(device)\n",
    "            output = model('en', input_1, attn_1, input_2, attn_2)\n",
    "            #print(output)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            predictions.append(predicted)\n",
    "    #print(predictions)\n",
    "    with torch.no_grad():\n",
    "        # for Arabic data\n",
    "        for input_1, attn_1, input_2, attn_2, labels in ar_test_loader:\n",
    "            #optimizer.zero_grad()\n",
    "            input_1 = input_1.to(device)\n",
    "            input_2 = input_2.to(device)\n",
    "            attn_1 = attn_1.to(device)\n",
    "            attn_2 = attn_2.to(device)\n",
    "            labels = labels.to(device)\n",
    "            output = model('ar', input_1, attn_1, input_2, attn_2)\n",
    "            #print(output)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            predictions.append(predicted)\n",
    "    \n",
    "    # Writingg the predictions to a text file\n",
    "    with open(f'/kaggle/working/pred_ep_{ep}.txt', 'w') as f:\n",
    "      for line in predictions:\n",
    "        for l in line.tolist():\n",
    "          #print(l)\n",
    "          y_test_pred.append(l)\n",
    "          f.write(str(l)+\"\\n\")\n",
    "    print(f'Printing Results for ***Epoch: {ep}***.......')\n",
    "    target_labels = list(set(g_labels)).sort()\n",
    "    y_true = g_labels\n",
    "    # calculating the performances on test data\n",
    "    acc, precision, recall, F1, report = calculate_performance(y_true, y_test_pred, target_labels)\n",
    "    result = str(\"{0:.4f}\".format(acc)) + \"\\t\" + str(\"{0:.4f}\".format(precision)) + \"\\t\" + str(\n",
    "        \"{0:.4f}\".format(recall)) + \"\\t\" + str(\"{0:.4f}\".format(F1)) + \"\\n\"\n",
    "\n",
    "    print(\"Test set:\\t Acc\\tPrecision\\tRecall\\tF1\\n\" + result)\n",
    "    print(report)\n",
    "    return predictions"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-20T22:27:08.878563Z",
     "iopub.execute_input": "2023-11-20T22:27:08.878840Z",
     "iopub.status.idle": "2023-11-20T22:27:08.900376Z",
     "shell.execute_reply.started": "2023-11-20T22:27:08.878817Z",
     "shell.execute_reply": "2023-11-20T22:27:08.899586Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reading the data from files"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "def read_dataset(file_loc, delim=',', lang='en'):\n",
    "    data = []\n",
    "    labels = []\n",
    "    with open(file_loc) as f:\n",
    "        reader = csv.reader(f, delimiter=delim)\n",
    "        next(reader)\n",
    "        i = 0\n",
    "        for row in reader:\n",
    "            data.append(row[1])\n",
    "            labels.append(int(row[2]))\n",
    "    return data, labels"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-20T22:27:08.901449Z",
     "iopub.execute_input": "2023-11-20T22:27:08.901723Z",
     "iopub.status.idle": "2023-11-20T22:27:08.913480Z",
     "shell.execute_reply.started": "2023-11-20T22:27:08.901699Z",
     "shell.execute_reply": "2023-11-20T22:27:08.912728Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Defining evaluation metric"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def calculate_performance(y_true, y_pred, labels):\n",
    "    \"\"\"\n",
    "    Calculating performances of our model\n",
    "    :param y_true: actual labels in test set\n",
    "    :param y_pred: predicted labels\n",
    "    :param labels:\n",
    "    :return: accuracy, precision, recall, f1 score and classification report\n",
    "    \"\"\"\n",
    "    (acc, P, R, F1) = (0.0, 0.0, 0.0, 0.0)\n",
    "    acc = metrics.accuracy_score(y_true, y_pred)\n",
    "    P = metrics.precision_score(y_true, y_pred, average='weighted')\n",
    "    R = metrics.recall_score(y_true, y_pred, average='weighted')\n",
    "    F1 = metrics.f1_score(y_true, y_pred, average='macro')\n",
    "    report = metrics.classification_report(y_true, y_pred, target_names=labels, digits=4)\n",
    "\n",
    "    return acc * 100, P * 100, R * 100, F1 * 100, report"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-20T22:27:08.914519Z",
     "iopub.execute_input": "2023-11-20T22:27:08.914780Z",
     "iopub.status.idle": "2023-11-20T22:27:09.366725Z",
     "shell.execute_reply.started": "2023-11-20T22:27:08.914757Z",
     "shell.execute_reply": "2023-11-20T22:27:09.365751Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reading local files and prepare the data for training"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#%cd ../1a_en/\n",
    "train_data_file = '/kaggle/input/semeval17/english/train.csv'\n",
    "dev_data_file = '/kaggle/input/semeval17/english/dev.csv'\n",
    "test_data_file = '/kaggle/input/semeval17/english/test.csv'\n",
    "file_delim = ','\n",
    "train_data_file_ar = '/kaggle/input/semeval17/arabic/train.csv'\n",
    "dev_data_file_ar = '/kaggle/input/semeval17/arabic/dev.csv'\n",
    "test_data_file_ar = '/kaggle/input/semeval17/arabic/test.csv'\n",
    "\n",
    "print(f\"Reading Train file: {train_data_file}\")\n",
    "en_data, en_labels = read_dataset(train_data_file, file_delim)\n",
    "en_train_dataset = EnsembleDataset(en_data, en_labels)\n",
    "en_train_dataset = torch.utils.data.DataLoader(en_train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "ar_data, ar_labels = read_dataset(train_data_file_ar, file_delim)\n",
    "ar_train_dataset = EnsembleDataset(ar_data, ar_labels, 'ar')\n",
    "ar_train_dataset = torch.utils.data.DataLoader(ar_train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "print(f'Total examples in arabic train set: {len(ar_data)}')\n",
    "print(f'Total examples in english train set: {len(en_data)}')\n",
    "\n",
    "\n",
    "print(f\"Reading Dev file: {dev_data_file}\")\n",
    "en_data, en_labels = read_dataset(dev_data_file, file_delim)\n",
    "en_dev_dataset = EnsembleDataset(en_data, en_labels)\n",
    "en_dev_dataset = torch.utils.data.DataLoader(en_dev_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "ar_data, ar_labels = read_dataset(dev_data_file_ar, file_delim)\n",
    "ar_dev_dataset = EnsembleDataset(ar_data, ar_labels, 'ar')\n",
    "ar_dev_dataset = torch.utils.data.DataLoader(ar_dev_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "\n",
    "print(f\"Reading Test file: {test_data_file}\")\n",
    "en_data, en_labels = read_dataset(test_data_file, file_delim)\n",
    "en_test_dataset = EnsembleDataset(en_data, en_labels)\n",
    "en_test_dataset = torch.utils.data.DataLoader(en_test_dataset, batch_size=BATCH_SIZE)\n",
    "ar_data, ar_labels = read_dataset(test_data_file_ar, file_delim)\n",
    "ar_test_dataset = EnsembleDataset(ar_data, ar_labels, 'ar')\n",
    "ar_test_dataset = torch.utils.data.DataLoader(ar_test_dataset, batch_size=BATCH_SIZE)\n",
    "ts_labels = en_labels + ar_labels\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-20T22:27:09.368464Z",
     "iopub.execute_input": "2023-11-20T22:27:09.368818Z",
     "iopub.status.idle": "2023-11-20T22:27:39.846714Z",
     "shell.execute_reply.started": "2023-11-20T22:27:09.368785Z",
     "shell.execute_reply": "2023-11-20T22:27:39.845928Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Defining model, loss function, and optimizers"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = EnsembleClassifier(num_classes=3)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-20T22:27:39.849035Z",
     "iopub.execute_input": "2023-11-20T22:27:39.849336Z",
     "iopub.status.idle": "2023-11-20T22:28:32.691806Z",
     "shell.execute_reply.started": "2023-11-20T22:27:39.849309Z",
     "shell.execute_reply": "2023-11-20T22:28:32.690799Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### train and validate the model"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "num_epochs = 2\n",
    "for epoch in range(0,num_epochs):\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    en_train_loss, en_corr = train(model, en_train_dataset, criterion, optimizer, device,'en')\n",
    "    ar_train_loss, ar_corr = train(model, ar_train_dataset, criterion, optimizer, device,'ar')\n",
    "    train_loss = (en_train_loss + ar_train_loss) / (len(en_train_dataset.dataset) + len(ar_train_dataset.dataset))\n",
    "    train_acc = (en_corr + ar_corr) / (len(en_train_dataset.dataset) + len(ar_train_dataset.dataset))\n",
    "    en_dev_loss, en_corr = test(model, en_dev_dataset, criterion, device, 'en')\n",
    "    ar_dev_loss, ar_corr = test(model, ar_dev_dataset, criterion, device, 'ar')\n",
    "    dev_loss = (en_dev_loss + ar_dev_loss) / (len(en_dev_dataset.dataset) + len(ar_dev_dataset.dataset))\n",
    "    dev_acc = (en_corr + ar_corr) / (len(en_dev_dataset.dataset) + len(ar_dev_dataset.dataset))\n",
    "    print('Epoch {}/{}: Train Loss = {:.4f}, Accuracy = {:.4f}, Dev Loss = {:.4f}, Dev Accuracy = {:.4f}'.format(epoch+1, num_epochs, train_loss, train_acc, dev_loss, dev_acc))\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-20T22:28:32.693489Z",
     "iopub.execute_input": "2023-11-20T22:28:32.693945Z",
     "iopub.status.idle": "2023-11-21T04:40:48.634071Z",
     "shell.execute_reply.started": "2023-11-20T22:28:32.693918Z",
     "shell.execute_reply": "2023-11-21T04:40:48.633082Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate the model"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "test_prediction = evaluate(model, [en_test_dataset, ar_test_dataset], device, epoch, ts_labels)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}