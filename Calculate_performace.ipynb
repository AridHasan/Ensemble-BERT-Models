{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Reading the gold test labels for Arabic and English"
   ],
   "metadata": {
    "id": "bIdCufU6O5zS",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "en_gold_filepath = './data/english/test.csv'\n",
    "ar_gold_filepath = './data/arabic/test.csv'\n",
    "\n",
    "en_gold_labels = []\n",
    "ar_gold_labels = []\n",
    "\n",
    "with open(en_gold_filepath) as f:\n",
    "  reader = csv.reader(f)\n",
    "  next(reader)\n",
    "  for row in reader:\n",
    "    en_gold_labels.append(row[2])\n",
    "\n",
    "with open(ar_gold_filepath) as f:\n",
    "  reader = csv.reader(f)\n",
    "  next(reader)\n",
    "  for row in reader:\n",
    "    ar_gold_labels.append(row[2])"
   ],
   "metadata": {
    "id": "WZHtV8byFR4z",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Defining evaluation metrics"
   ],
   "metadata": {
    "id": "LC4sy2wEPAI0",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def calculate_performance(y_true, y_pred, labels):\n",
    "    \"\"\"\n",
    "    Calculating performances of our model\n",
    "    :param y_true: actual labels in test set\n",
    "    :param y_pred: predicted labels\n",
    "    :param labels:\n",
    "    :return: accuracy, precision, recall, f1 score and classification report\n",
    "    \"\"\"\n",
    "    (acc, P, R, F1) = (0.0, 0.0, 0.0, 0.0)\n",
    "    acc = metrics.accuracy_score(y_true, y_pred)\n",
    "    P = metrics.precision_score(y_true, y_pred, average='weighted')\n",
    "    R = metrics.recall_score(y_true, y_pred, average='weighted')\n",
    "    F1 = metrics.f1_score(y_true, y_pred, average='macro')\n",
    "    report = metrics.classification_report(y_true, y_pred, target_names=labels, digits=4)\n",
    "\n",
    "    return acc * 100, P * 100, R * 100, F1 * 100, report"
   ],
   "metadata": {
    "id": "ZAT7cdx_eQP0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generate performances for finetuning models"
   ],
   "metadata": {
    "id": "jCm8IRG6PFjF",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "en_models = ['robert-en', 'mbert-en', 'xlm-en']\n",
    "ar_models = ['arabert-ar', 'mbert-ar', 'xlm-ar']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "for model in en_models:\n",
    "  predicted_labels = []\n",
    "  print(f'Reading predicted data for: {model}')\n",
    "  lines = open(model+'/predict_results.txt').read().strip().split('\\n')\n",
    "  for line in lines[1:]:\n",
    "    id, label = line.split('\\t')\n",
    "    predicted_labels.append(label.strip())\n",
    "  acc, precision, recall, F1, report = calculate_performance(en_gold_labels, predicted_labels, ['0', '1', '2'])\n",
    "  result = str(\"{0:.4f}\".format(acc)) + \"\\t\" + str(\"{0:.4f}\".format(precision)) + \"\\t\" + str(\n",
    "        \"{0:.4f}\".format(recall)) + \"\\t\" + str(\"{0:.4f}\".format(F1)) + \"\\n\"\n",
    "\n",
    "  print(\"Test set:\\t Acc\\tPrecision\\tRecall\\tF1\\n\" + result)\n",
    "  print(report)\n",
    "  print('-'*25)\n",
    "\n",
    "for model in ar_models:\n",
    "  predicted_labels = []\n",
    "  print(f'Reading predicted data for: {model}')\n",
    "  lines = open(model+'/predict_results.txt').read().strip().split('\\n')\n",
    "  for line in lines[1:]:\n",
    "    id, label = line.split('\\t')\n",
    "    predicted_labels.append(label.strip())\n",
    "  acc, precision, recall, F1, report = calculate_performance(ar_gold_labels, predicted_labels, ['0', '1', '2'])\n",
    "  result = str(\"{0:.4f}\".format(acc)) + \"\\t\" + str(\"{0:.4f}\".format(precision)) + \"\\t\" + str(\n",
    "        \"{0:.4f}\".format(recall)) + \"\\t\" + str(\"{0:.4f}\".format(F1)) + \"\\n\"\n",
    "\n",
    "  print(\"Test set:\\t Acc\\tPrecision\\tRecall\\tF1\\n\" + result)\n",
    "  print(report)\n",
    "  print('-'*25)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0SWZTh0Sdrlw",
    "outputId": "f0856ff4-9947-4dc8-fc9b-68793372d9d3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading predicted data for: robert-en\n",
      "Test set:\t Acc\tPrecision\tRecall\tF1\n",
      "70.6936\t71.3441\t70.6936\t70.8404\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7085    0.7681    0.7371      3972\n",
      "           1     0.7489    0.6345    0.6870      5937\n",
      "           2     0.6330    0.7857    0.7011      2375\n",
      "\n",
      "    accuracy                         0.7069     12284\n",
      "   macro avg     0.6968    0.7294    0.7084     12284\n",
      "weighted avg     0.7134    0.7069    0.7059     12284\n",
      "\n",
      "-------------------------\n",
      "Reading predicted data for: mbert-en\n",
      "Test set:\t Acc\tPrecision\tRecall\tF1\n",
      "67.1605\t67.4815\t67.1605\t67.0634\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6783    0.6981    0.6881      3972\n",
      "           1     0.6988    0.6367    0.6663      5937\n",
      "           2     0.6089    0.7145    0.6575      2375\n",
      "\n",
      "    accuracy                         0.6716     12284\n",
      "   macro avg     0.6620    0.6831    0.6706     12284\n",
      "weighted avg     0.6748    0.6716    0.6717     12284\n",
      "\n",
      "-------------------------\n",
      "Reading predicted data for: xlm-en\n",
      "Test set:\t Acc\tPrecision\tRecall\tF1\n",
      "69.0736\t69.9966\t69.0736\t69.1314\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6902    0.7651    0.7257      3972\n",
      "           1     0.7442    0.6087    0.6697      5937\n",
      "           2     0.6056    0.7714    0.6785      2375\n",
      "\n",
      "    accuracy                         0.6907     12284\n",
      "   macro avg     0.6800    0.7151    0.6913     12284\n",
      "weighted avg     0.7000    0.6907    0.6895     12284\n",
      "\n",
      "-------------------------\n",
      "Reading predicted data for: arabert-ar\n",
      "Test set:\t Acc\tPrecision\tRecall\tF1\n",
      "69.7869\t69.9608\t69.7869\t69.7756\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7975    0.7691    0.7830      2222\n",
      "           1     0.6366    0.6261    0.6313      2364\n",
      "           2     0.6544    0.7054    0.6790      1514\n",
      "\n",
      "    accuracy                         0.6979      6100\n",
      "   macro avg     0.6962    0.7002    0.6978      6100\n",
      "weighted avg     0.6996    0.6979    0.6984      6100\n",
      "\n",
      "-------------------------\n",
      "Reading predicted data for: mbert-ar\n",
      "Test set:\t Acc\tPrecision\tRecall\tF1\n",
      "54.2131\t53.7634\t54.2131\t53.0766\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5624    0.7426    0.6400      2222\n",
      "           1     0.5239    0.3794    0.4401      2364\n",
      "           2     0.5227    0.5020    0.5121      1514\n",
      "\n",
      "    accuracy                         0.5421      6100\n",
      "   macro avg     0.5363    0.5413    0.5308      6100\n",
      "weighted avg     0.5376    0.5421    0.5308      6100\n",
      "\n",
      "-------------------------\n",
      "Reading predicted data for: xlm-ar\n",
      "Test set:\t Acc\tPrecision\tRecall\tF1\n",
      "63.8852\t63.6276\t63.8852\t63.7358\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6873    0.7480    0.7164      2222\n",
      "           1     0.6071    0.5097    0.5542      2364\n",
      "           2     0.6070    0.6803    0.6415      1514\n",
      "\n",
      "    accuracy                         0.6389      6100\n",
      "   macro avg     0.6338    0.6460    0.6374      6100\n",
      "weighted avg     0.6363    0.6389    0.6349      6100\n",
      "\n",
      "-------------------------\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Calculate majority voting performance"
   ],
   "metadata": {
    "id": "yxHjauxfvkyi",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "def get_majority_label(data):\n",
    "  label, count = Counter(data).most_common()[0]\n",
    "  return label"
   ],
   "metadata": {
    "id": "RHS936SpweJx",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### English"
   ],
   "metadata": {
    "id": "nCFAK5Dl2Bzh",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "predicted_labels = {}\n",
    "for model in en_models:\n",
    "  print(f'Reading predicted data for: {model}')\n",
    "  lines = open(model+'/predict_results.txt').read().strip().split('\\n')\n",
    "  for line in lines[1:]:\n",
    "    id, label = line.split('\\t')\n",
    "    if id not in predicted_labels:\n",
    "      predicted_labels[id] = []\n",
    "    predicted_labels[id].append(label.strip())\n",
    "\n",
    "final_pred_labels = []\n",
    "for k, v in predicted_labels.items():\n",
    "  final_pred_labels.append(get_majority_label(v))\n",
    "\n",
    "acc, precision, recall, F1, report = calculate_performance(en_gold_labels, final_pred_labels, ['0', '1', '2'])\n",
    "result = str(\"{0:.4f}\".format(acc)) + \"\\t\" + str(\"{0:.4f}\".format(precision)) + \"\\t\" + str(\n",
    "        \"{0:.4f}\".format(recall)) + \"\\t\" + str(\"{0:.4f}\".format(F1)) + \"\\n\"\n",
    "\n",
    "print(\"Test set:\\t Acc\\tPrecision\\tRecall\\tF1\\n\" + result)\n",
    "print(report)"
   ],
   "metadata": {
    "id": "ooXaqI9d2Fys",
    "outputId": "3d214c3c-bc75-452d-d69c-f3b61c60c798",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading predicted data for: robert-en\n",
      "Reading predicted data for: mbert-en\n",
      "Reading predicted data for: xlm-en\n",
      "Test set:\t Acc\tPrecision\tRecall\tF1\n",
      "70.9541\t71.5480\t70.9541\t71.0322\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7121    0.7666    0.7384      3972\n",
      "           1     0.7498    0.6434    0.6925      5937\n",
      "           2     0.6354    0.7794    0.7001      2375\n",
      "\n",
      "    accuracy                         0.7095     12284\n",
      "   macro avg     0.6991    0.7298    0.7103     12284\n",
      "weighted avg     0.7155    0.7095    0.7088     12284\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Arabic"
   ],
   "metadata": {
    "id": "boIJUMmy1_bS",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "predicted_labels = {}\n",
    "for model in ar_models:\n",
    "  print(f'Reading predicted data for: {model}')\n",
    "  lines = open(model+'/predict_results.txt').read().strip().split('\\n')\n",
    "  for line in lines[1:]:\n",
    "    id, label = line.split('\\t')\n",
    "    if id not in predicted_labels:\n",
    "      predicted_labels[id] = []\n",
    "    predicted_labels[id].append(label.strip())\n",
    "\n",
    "final_pred_labels = []\n",
    "for k, v in predicted_labels.items():\n",
    "  final_pred_labels.append(get_majority_label(v))\n",
    "\n",
    "acc, precision, recall, F1, report = calculate_performance(ar_gold_labels, final_pred_labels, ['0', '1', '2'])\n",
    "result = str(\"{0:.4f}\".format(acc)) + \"\\t\" + str(\"{0:.4f}\".format(precision)) + \"\\t\" + str(\n",
    "        \"{0:.4f}\".format(recall)) + \"\\t\" + str(\"{0:.4f}\".format(F1)) + \"\\n\"\n",
    "\n",
    "print(\"Test set:\\t Acc\\tPrecision\\tRecall\\tF1\\n\" + result)\n",
    "print(report)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OxdvUF2ZrY3p",
    "outputId": "1fceda8d-4604-4ea2-ac0d-e5f5201e6a71",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading predicted data for: arabert-ar\n",
      "Reading predicted data for: mbert-ar\n",
      "Reading predicted data for: xlm-ar\n",
      "Test set:\t Acc\tPrecision\tRecall\tF1\n",
      "66.6885\t66.3713\t66.6885\t66.4150\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7120    0.7921    0.7499      2222\n",
      "           1     0.6354    0.5381    0.5827      2364\n",
      "           2     0.6371    0.6843    0.6599      1514\n",
      "\n",
      "    accuracy                         0.6669      6100\n",
      "   macro avg     0.6615    0.6715    0.6642      6100\n",
      "weighted avg     0.6637    0.6669    0.6628      6100\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Calculating the performance of BERTs-FF Ensemble"
   ],
   "metadata": {
    "id": "0PEaFF3rE0vc",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### with attention mechanism"
   ],
   "metadata": {
    "id": "D56WvTfuFBa5",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import glob\n",
    "path = './ensemble_attn/'\n",
    "\n",
    "for filepath in glob.glob(f'{path}*.txt'):\n",
    "  predicted_labels = []\n",
    "  print(f'Reading predicted data from: {filepath}')\n",
    "  lines = open(filepath).read().strip().split('\\n')\n",
    "  for line in lines:\n",
    "    predicted_labels.append(line.strip())\n",
    "  en_pred_labels = predicted_labels[:len(en_gold_labels)]\n",
    "  ar_pred_labels = predicted_labels[len(en_gold_labels):]\n",
    "  print('English Data: ')\n",
    "  acc, precision, recall, F1, report = calculate_performance(en_gold_labels, en_pred_labels, ['0', '1', '2'])\n",
    "  result = str(\"{0:.4f}\".format(acc)) + \"\\t\" + str(\"{0:.4f}\".format(precision)) + \"\\t\" + str(\n",
    "          \"{0:.4f}\".format(recall)) + \"\\t\" + str(\"{0:.4f}\".format(F1)) + \"\\n\"\n",
    "\n",
    "  print(\"Test set:\\t Acc\\tPrecision\\tRecall\\tF1\\n\" + result)\n",
    "  print(report)\n",
    "  print('Arabic Data: ')\n",
    "  acc, precision, recall, F1, report = calculate_performance(ar_gold_labels, ar_pred_labels, ['0', '1', '2'])\n",
    "  result = str(\"{0:.4f}\".format(acc)) + \"\\t\" + str(\"{0:.4f}\".format(precision)) + \"\\t\" + str(\n",
    "          \"{0:.4f}\".format(recall)) + \"\\t\" + str(\"{0:.4f}\".format(F1)) + \"\\n\"\n",
    "\n",
    "  print(\"Test set:\\t Acc\\tPrecision\\tRecall\\tF1\\n\" + result)\n",
    "  print(report)"
   ],
   "metadata": {
    "id": "lnjQq9ASxHTY",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0d3163f2-e7c4-403d-bdb5-ab415e2b8e42",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading predicted data from: ./ensemble_attn/output-pred_ep_2.txt\n",
      "English Data: \n",
      "Test set:\t Acc\tPrecision\tRecall\tF1\n",
      "67.4373\t69.1370\t67.4373\t67.3072\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6705    0.7548    0.7102      3972\n",
      "           1     0.7562    0.5826    0.6582      5937\n",
      "           2     0.5641    0.7693    0.6509      2375\n",
      "\n",
      "    accuracy                         0.6744     12284\n",
      "   macro avg     0.6636    0.7022    0.6731     12284\n",
      "weighted avg     0.6914    0.6744    0.6736     12284\n",
      "\n",
      "Arabic Data: \n",
      "Test set:\t Acc\tPrecision\tRecall\tF1\n",
      "66.2951\t67.8214\t66.2951\t66.4180\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8254    0.6787    0.7449      2222\n",
      "           1     0.5997    0.6324    0.6156      2364\n",
      "           2     0.5848    0.6876    0.6321      1514\n",
      "\n",
      "    accuracy                         0.6630      6100\n",
      "   macro avg     0.6700    0.6662    0.6642      6100\n",
      "weighted avg     0.6782    0.6630    0.6668      6100\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### with Feed Forward"
   ],
   "metadata": {
    "id": "bT2DP5QPJg9-",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import glob\n",
    "path = './ensemble-ff/'\n",
    "\n",
    "for filepath in glob.glob(f'{path}*.txt'):\n",
    "  predicted_labels = []\n",
    "  print(f'Reading predicted data from: {filepath}')\n",
    "  lines = open(filepath).read().strip().split('\\n')\n",
    "  for line in lines:\n",
    "    predicted_labels.append(line.strip())\n",
    "  en_pred_labels = predicted_labels[:len(en_gold_labels)]\n",
    "  ar_pred_labels = predicted_labels[len(en_gold_labels):]\n",
    "  print('English Data: ')\n",
    "  acc, precision, recall, F1, report = calculate_performance(en_gold_labels, en_pred_labels, ['0', '1', '2'])\n",
    "  result = str(\"{0:.4f}\".format(acc)) + \"\\t\" + str(\"{0:.4f}\".format(precision)) + \"\\t\" + str(\n",
    "          \"{0:.4f}\".format(recall)) + \"\\t\" + str(\"{0:.4f}\".format(F1)) + \"\\n\"\n",
    "\n",
    "  print(\"Test set:\\t Acc\\tPrecision\\tRecall\\tF1\\n\" + result)\n",
    "  print(report)\n",
    "  print('Arabic Data: ')\n",
    "  acc, precision, recall, F1, report = calculate_performance(ar_gold_labels, ar_pred_labels, ['0', '1', '2'])\n",
    "  result = str(\"{0:.4f}\".format(acc)) + \"\\t\" + str(\"{0:.4f}\".format(precision)) + \"\\t\" + str(\n",
    "          \"{0:.4f}\".format(recall)) + \"\\t\" + str(\"{0:.4f}\".format(F1)) + \"\\n\"\n",
    "\n",
    "  print(\"Test set:\\t Acc\\tPrecision\\tRecall\\tF1\\n\" + result)\n",
    "  print(report)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "toHEgrVWGuAo",
    "outputId": "2c1f605b-68be-4eeb-883a-995d2a297690",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading predicted data from: ./ensemble-ff/pred_ep_2.txt\n",
      "English Data: \n",
      "Test set:\t Acc\tPrecision\tRecall\tF1\n",
      "70.0261\t70.4980\t70.0261\t69.8751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6758    0.7815    0.7248      3972\n",
      "           1     0.7435    0.6426    0.6894      5937\n",
      "           2     0.6574    0.7086    0.6821      2375\n",
      "\n",
      "    accuracy                         0.7003     12284\n",
      "   macro avg     0.6923    0.7109    0.6988     12284\n",
      "weighted avg     0.7050    0.7003    0.6994     12284\n",
      "\n",
      "Arabic Data: \n",
      "Test set:\t Acc\tPrecision\tRecall\tF1\n",
      "67.6066\t68.0116\t67.6066\t67.1159\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7723    0.7678    0.7700      2222\n",
      "           1     0.5928    0.6595    0.6243      2364\n",
      "           2     0.6812    0.5674    0.6191      1514\n",
      "\n",
      "    accuracy                         0.6761      6100\n",
      "   macro avg     0.6821    0.6649    0.6712      6100\n",
      "weighted avg     0.6801    0.6761    0.6761      6100\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### English as train data and English as test data"
   ],
   "metadata": {
    "id": "1j2mVbdASz1i",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import glob\n",
    "path = './test101/'\n",
    "\n",
    "for filepath in glob.glob(f'{path}*.txt'):\n",
    "  predicted_labels = []\n",
    "  print(f'Reading predicted data from: {filepath}')\n",
    "  lines = open(filepath).read().strip().split('\\n')\n",
    "  for line in lines:\n",
    "    predicted_labels.append(line.strip())\n",
    "  print('English Data: ')\n",
    "  acc, precision, recall, F1, report = calculate_performance(en_gold_labels, predicted_labels, ['0', '1', '2'])\n",
    "  result = str(\"{0:.4f}\".format(acc)) + \"\\t\" + str(\"{0:.4f}\".format(precision)) + \"\\t\" + str(\n",
    "          \"{0:.4f}\".format(recall)) + \"\\t\" + str(\"{0:.4f}\".format(F1)) + \"\\n\"\n",
    "\n",
    "  print(\"Test set:\\t Acc\\tPrecision\\tRecall\\tF1\\n\" + result)\n",
    "  print(report)"
   ],
   "metadata": {
    "id": "sK45TrS1JnTC",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a84eb2b5-0f88-4f51-e491-da4b7ffd15fe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading predicted data from: ./test101/output-pred_ep_2.txt\n",
      "English Data: \n",
      "Test set:\t Acc\tPrecision\tRecall\tF1\n",
      "68.9108\t69.2574\t68.9108\t68.5915\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7245    0.6548    0.6879      3972\n",
      "           1     0.6995    0.6945    0.6970      5937\n",
      "           2     0.6218    0.7331    0.6729      2375\n",
      "\n",
      "    accuracy                         0.6891     12284\n",
      "   macro avg     0.6819    0.6941    0.6859     12284\n",
      "weighted avg     0.6926    0.6891    0.6894     12284\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Arabic as train data and Arabic as test data"
   ],
   "metadata": {
    "id": "_-Uui1ZaS7bk",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import glob\n",
    "path = './test102/'\n",
    "\n",
    "for filepath in glob.glob(f'{path}*.txt'):\n",
    "  predicted_labels = []\n",
    "  print(f'Reading predicted data from: {filepath}')\n",
    "  lines = open(filepath).read().strip().split('\\n')\n",
    "  for line in lines:\n",
    "    predicted_labels.append(line.strip())\n",
    "  print('Arabic Data: ')\n",
    "  acc, precision, recall, F1, report = calculate_performance(ar_gold_labels, predicted_labels, ['0', '1', '2'])\n",
    "  result = str(\"{0:.4f}\".format(acc)) + \"\\t\" + str(\"{0:.4f}\".format(precision)) + \"\\t\" + str(\n",
    "          \"{0:.4f}\".format(recall)) + \"\\t\" + str(\"{0:.4f}\".format(F1)) + \"\\n\"\n",
    "\n",
    "  print(\"Test set:\\t Acc\\tPrecision\\tRecall\\tF1\\n\" + result)\n",
    "  print(report)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kmxicJPIS6js",
    "outputId": "67c15d20-437d-4442-ad67-22bfc515d0fe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading predicted data from: ./test102/output-pred_ep_2.txt\n",
      "Arabic Data: \n",
      "Test set:\t Acc\tPrecision\tRecall\tF1\n",
      "67.6721\t69.0103\t67.6721\t67.8239\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8167    0.6796    0.7418      2222\n",
      "           1     0.5954    0.6853    0.6372      2364\n",
      "           2     0.6523    0.6592    0.6557      1514\n",
      "\n",
      "    accuracy                         0.6767      6100\n",
      "   macro avg     0.6881    0.6747    0.6782      6100\n",
      "weighted avg     0.6901    0.6767    0.6799      6100\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}